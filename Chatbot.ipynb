{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ksvVIgO2QEXcAB6Oeuf8f5IR2JchwcS5",
      "authorship_tag": "ABX9TyNxFfwapzM0qCiuagFiwqpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheJv21/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ZVoTE_xYXMTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkWJfhddvO6X"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#Para crear la red neuronal\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from nltk.stem import WordNetLemmatizer  #Para pasar las palabras a su forma raíz"
      ],
      "metadata": {
        "id": "KU2RlfNLOScf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "intents = json.loads(open('/content/intent.json').read())\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '¿', '.', ',']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqObH2_TOfBG",
        "outputId": "99175d9a-2dee-4fb1-b0b0-e3d632022b14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clasifica los patrones y las categorías\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent[\"tag\"]))\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])\n",
        "\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "wekCyorGTs9M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pasa la información a unos y ceros según las palabras presentes en cada categoría para hacer el entrenamiento\n",
        "training = []\n",
        "output_empty = [0]*len(classes)\n",
        "for document in documents:\n",
        "    bag = []\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "print(training)"
      ],
      "metadata": {
        "id": "EBEbZyA9US96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reparte los datos para pasarlos a la red\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "_ttsXS7vUXW0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la red neuronal\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))"
      ],
      "metadata": {
        "id": "OJ1oRKtKUanm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos el optimizador y lo compilamos\n",
        "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n"
      ],
      "metadata": {
        "id": "cRz7TZJtUeXw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamos el modelo y lo guardamos\n",
        "train_process = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=5, verbose=1)\n",
        "model.save(\"chatbot_model.h5\", train_process)"
      ],
      "metadata": {
        "id": "MgaeUCJEVa2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "TyQhDJcIWGoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "N2pxsUeeWKnY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos los archivos generados en el código anterior\n",
        "intents = json.loads(open('/content/intent.json').read())\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "model = load_model('chatbot_model.h5')"
      ],
      "metadata": {
        "id": "cLXJfgl5Wmqp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pasamos las palabras de oración a su forma raíz\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    return sentence_words"
      ],
      "metadata": {
        "id": "AnBhDP9aWt_c"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertimos la información a unos y ceros según si están presentes en los patrones\n",
        "def bag_of_words(sentence):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i]=1\n",
        "    print(bag)\n",
        "    return np.array(bag)"
      ],
      "metadata": {
        "id": "ahvXz6BUWwPK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predecimos la categoría a la que pertenece la oración\n",
        "def predict_class(sentence):\n",
        "    bow = bag_of_words(sentence)\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "    max_index = np.where(res ==np.max(res))[0][0]\n",
        "    category = classes[max_index]\n",
        "    return category"
      ],
      "metadata": {
        "id": "URaY9b2PWyvg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenemos una respuesta aleatoria\n",
        "def get_response(tag, intents_json):\n",
        "    list_of_intents = intents_json['intents']\n",
        "    result = \"\"\n",
        "    for i in list_of_intents:\n",
        "        if i[\"tag\"]==tag:\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result"
      ],
      "metadata": {
        "id": "4eA2IVtFW0ue"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejecutamos el chat en bucle\n",
        "while True:\n",
        "    message=input(\"\")\n",
        "    ints = predict_class(message)\n",
        "    res = get_response(ints, intents)\n",
        "    print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEaolCTZW2am",
        "outputId": "328c284c-d8a2-4251-b258-7b5101701d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nombre\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "¿Cómo estás?\n",
            "como te llamas\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Mi nombre es Juan Pablo\n",
            "hola\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "¿Cómo estás?\n"
          ]
        }
      ]
    }
  ]
}